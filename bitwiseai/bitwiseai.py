# -*- coding: utf-8 -*-
"""
BitwiseAI - ç¡¬ä»¶è°ƒè¯•å’Œæ—¥å¿—åˆ†æçš„ AI å·¥å…·

ä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯ã€æ—¥å¿—è§£æå’Œæ™ºèƒ½åˆ†æ
åŸºäº LangChainï¼Œæ”¯æŒæœ¬åœ° Milvus å‘é‡æ•°æ®åº“
"""
import os
import json
from typing import List, Optional, Dict, Any, Union, Callable
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from .llm import LLM
from .embedding import Embedding
from .vector_database import MilvusDB
from .utils import DocumentLoader, TextSplitter
from .tools import ToolRegistry, register_builtin_tools
from .interfaces import TaskInterface, AnalysisResult
from .reporter import Reporter


class BitwiseAI:
    """
    BitwiseAI æ ¸å¿ƒç±»

    ç‰¹æ€§ï¼š
    - ç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æ
    - åŸºäº LangChain çš„ ChatOpenAI å’Œ OpenAIEmbeddings
    - æœ¬åœ° Milvus å‘é‡æ•°æ®åº“
    - æ”¯æŒ RAG æ¨¡å¼å’Œçº¯ LLM æ¨¡å¼
    - è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯å’Œå·¥å…·æ‰©å±•
    """

    def __init__(
        self,
        config_path: str = "~/.bitwiseai/config.json"
    ):
        """
        åˆå§‹åŒ– BitwiseAI

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # å±•å¼€è·¯å¾„
        self.config_path = os.path.expanduser(config_path)
        self.working_dir = os.path.dirname(self.config_path)

        # åŠ è½½é…ç½®
        self.config = self._load_config()

        # è·å– LLM API é…ç½®ï¼ˆä¼˜å…ˆä»é…ç½®æ–‡ä»¶ï¼Œå…¶æ¬¡ä»ç¯å¢ƒå˜é‡ï¼‰
        llm_config = self.config.get("llm", {})
        llm_api_key = llm_config.get("api_key") or os.getenv("LLM_API_KEY")
        llm_base_url = llm_config.get("base_url") or os.getenv("LLM_BASE_URL")

        if not llm_api_key:
            raise ValueError("è¯·åœ¨é…ç½®æ–‡ä»¶æˆ– .env æ–‡ä»¶ä¸­è®¾ç½® LLM API Key")
        if not llm_base_url:
            raise ValueError("è¯·åœ¨é…ç½®æ–‡ä»¶æˆ– .env æ–‡ä»¶ä¸­è®¾ç½® LLM Base URL")

        # è·å– Embedding API é…ç½®ï¼ˆä¼˜å…ˆä»é…ç½®æ–‡ä»¶ï¼Œå…¶æ¬¡ä»ç¯å¢ƒå˜é‡ï¼‰
        embedding_config = self.config.get("embedding", {})
        embedding_api_key = embedding_config.get("api_key") or os.getenv("EMBEDDING_API_KEY")
        embedding_base_url = embedding_config.get("base_url") or os.getenv("EMBEDDING_BASE_URL")

        if not embedding_api_key:
            raise ValueError("è¯·åœ¨é…ç½®æ–‡ä»¶æˆ– .env æ–‡ä»¶ä¸­è®¾ç½® Embedding API Key")
        if not embedding_base_url:
            raise ValueError("è¯·åœ¨é…ç½®æ–‡ä»¶æˆ– .env æ–‡ä»¶ä¸­è®¾ç½® Embedding Base URL")

        # åˆå§‹åŒ– LLM
        self.llm = LLM(
            model=llm_config.get("model", "MiniMax-M2.1"),
            api_key=llm_api_key,
            base_url=llm_base_url,
            temperature=llm_config.get("temperature", 0.7)
        )

        # åˆå§‹åŒ– Embedding
        self.embedding = Embedding(
            model=embedding_config.get("model", "Qwen/Qwen3-Embedding-8B"),
            api_key=embedding_api_key,
            base_url=embedding_base_url
        )

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        vector_config = self.config.get("vector_db", {})
        db_file = os.path.expanduser(vector_config.get("db_file", "~/.bitwiseai/milvus_data.db"))
        collection_name = vector_config.get("collection_name", "bitwiseai")
        embedding_dim = vector_config.get("embedding_dim", 4096)

        self.vector_db = MilvusDB(
            db_file=db_file,
            embedding_model=self.embedding,
            collection_name=collection_name,
            embedding_dim=embedding_dim
        )

        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = self.config.get("system_prompt", "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚")

        # æ–‡æ¡£åŠ è½½å™¨å’Œåˆ‡åˆ†å™¨
        self.document_loader = DocumentLoader()
        self.text_splitter = TextSplitter()
        
        # å·¥å…·æ³¨å†Œå™¨
        self.tool_registry = ToolRegistry()
        register_builtin_tools(self.tool_registry)
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å·¥å…·
        if "tools" in self.config and self.config["tools"]:
            for tool_config in self.config["tools"]:
                try:
                    self.tool_registry.register_from_config(tool_config)
                except Exception as e:
                    print(f"è­¦å‘Š: åŠ è½½å·¥å…·å¤±è´¥: {str(e)}")
        
        # ä»»åŠ¡ç®¡ç†
        self.tasks: List[TaskInterface] = []
        self.task_results: Dict[str, List[AnalysisResult]] = {}
        
        # æŠ¥å‘Šç”Ÿæˆå™¨
        self.reporter = Reporter()
        
        # å½“å‰æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä»»åŠ¡æ‰§è¡Œï¼‰
        self.log_file_path: Optional[str] = None

        print("=" * 50)
        print("BitwiseAI åˆå§‹åŒ–å®Œæˆ")
        print(f"  LLM æ¨¡å‹: {llm_config.get('model')}")
        print(f"  Embedding æ¨¡å‹: {embedding_config.get('model')}")
        print(f"  å‘é‡åº“: {db_file}")
        print(f"  é›†åˆ: {collection_name}")
        print(f"  å·²æ³¨å†Œå·¥å…·: {len(self.tool_registry.list_tools())}")
        print("=" * 50)

    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def set_system_prompt(self, prompt: str):
        """
        è®¾ç½®ç³»ç»Ÿæç¤ºè¯

        Args:
            prompt: ç³»ç»Ÿæç¤ºè¯å†…å®¹
        """
        self.system_prompt = prompt
        print(f"ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°: {prompt[:50]}...")

    def load_documents(self, folder_path: str) -> int:
        """
        åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            åŠ è½½çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡
        """
        if not os.path.exists(folder_path):
            raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")

        print(f"ğŸ“‚ åŠ è½½æ–‡æ¡£ç›®å½•: {folder_path}")

        # åŠ è½½æ–‡æ¡£
        documents = self.document_loader.load_folder(folder_path)

        if not documents:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯åŠ è½½çš„æ–‡æ¡£")
            return 0

        # åˆ‡åˆ†æ–‡æ¡£
        chunks = []
        for doc in documents:
            doc_chunks = self.text_splitter.split(doc)
            chunks.extend(doc_chunks)

        print(f"ğŸ“„ æ‰¾åˆ° {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

        # æ’å…¥å‘é‡æ•°æ®åº“
        if chunks:
            self.vector_db.add_texts(chunks)
            print(f"âœ“ å·²æ’å…¥ {len(chunks)} ä¸ªç‰‡æ®µåˆ°å‘é‡åº“")

        return len(chunks)

    def add_text(self, text: str) -> int:
        """
        æ·»åŠ å•ä¸ªæ–‡æœ¬åˆ°å‘é‡æ•°æ®åº“

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            æ’å…¥çš„ç‰‡æ®µæ•°é‡
        """
        if not text or not text.strip():
            print("âš ï¸  æ–‡æœ¬å†…å®¹ä¸ºç©º")
            return 0

        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self.text_splitter.split(text)

        if chunks:
            self.vector_db.add_texts(chunks)
            print(f"âœ“ å·²æ’å…¥ {len(chunks)} ä¸ªç‰‡æ®µåˆ°å‘é‡åº“")

        return len(chunks)

    def chat(self, query: str, use_rag: bool = True) -> str:
        """
        å¯¹è¯æ–¹æ³•

        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ¨¡å¼ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            LLM ç”Ÿæˆçš„å›ç­”
        """
        if use_rag:
            return self._chat_with_rag(query)
        else:
            return self._chat_with_llm(query)

    def _chat_with_rag(self, query: str) -> str:
        """
        RAG æ¨¡å¼å¯¹è¯

        æ­¥éª¤ï¼š
        1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        2. æ„å»ºæç¤ºè¯
        3. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
        """
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = self.vector_db.search(query, top_k=5)

        if context:
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {query}

å›ç­”:"""
        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œé€€å›çº¯ LLM æ¨¡å¼
            print("âš ï¸  æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œé€€å›çº¯ LLM æ¨¡å¼")
            return self._chat_with_llm(query)

        # è°ƒç”¨ LLM
        return self.llm.invoke(prompt)

    def _chat_with_llm(self, query: str) -> str:
        """
        çº¯ LLM æ¨¡å¼å¯¹è¯
        """
        if self.system_prompt:
            prompt = f"{self.system_prompt}\n\nç”¨æˆ·: {query}"
        else:
            prompt = query

        return self.llm.invoke(prompt)

    def clear_vector_db(self):
        """
        æ¸…ç©ºå‘é‡æ•°æ®åº“
        """
        self.vector_db.clear()
        print("âœ“ å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
    
    # ========== å·¥å…·ç®¡ç† API ==========
    
    def register_tool(
        self,
        tool: Union[Callable, Dict[str, Any]],
        name: Optional[str] = None,
        description: str = ""
    ):
        """
        æ³¨å†Œè‡ªå®šä¹‰å·¥å…·
        
        Args:
            tool: å·¥å…·å¯¹è±¡ï¼Œå¯ä»¥æ˜¯ï¼š
                  - Python å‡½æ•°
                  - LangChain Tool
                  - é…ç½®å­—å…¸
            name: å·¥å…·åç§°ï¼ˆå¯é€‰ï¼‰
            description: å·¥å…·æè¿°ï¼ˆå¯é€‰ï¼‰
        
        ç¤ºä¾‹:
            # æ³¨å†Œ Python å‡½æ•°
            def my_parser(text):
                return text.split()
            ai.register_tool(my_parser, description="æ–‡æœ¬åˆ†å‰²å·¥å…·")
            
            # æ³¨å†Œé…ç½®åŒ–å·¥å…·
            ai.register_tool({
                "type": "shell_command",
                "name": "run_test",
                "command": "python test.py {input}",
                "description": "è¿è¡Œæµ‹è¯•"
            })
        """
        if callable(tool):
            self.tool_registry.register_function(tool, name, description)
        elif isinstance(tool, dict):
            self.tool_registry.register_from_config(tool)
        else:
            # å°è¯•ä½œä¸º LangChain Tool æ³¨å†Œ
            self.tool_registry.register_langchain_tool(tool, name)
        
        print(f"âœ“ å·¥å…·å·²æ³¨å†Œ: {name or getattr(tool, '__name__', 'unknown')}")
    
    def invoke_tool(self, name: str, *args, **kwargs) -> Any:
        """
        è°ƒç”¨å·²æ³¨å†Œçš„å·¥å…·
        
        Args:
            name: å·¥å…·åç§°
            *args: ä½ç½®å‚æ•°
            **kwargs: å…³é”®å­—å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        return self.tool_registry.invoke_tool(name, *args, **kwargs)
    
    def list_tools(self) -> List[str]:
        """
        åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„å·¥å…·
        
        Returns:
            å·¥å…·åç§°åˆ—è¡¨
        """
        return self.tool_registry.list_tools()
    
    # ========== ä»»åŠ¡ç®¡ç† API ==========
    
    def register_task(self, task: TaskInterface):
        """
        æ³¨å†Œåˆ†æä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡å¯¹è±¡ï¼Œå®ç° TaskInterface æ¥å£
            
        ç¤ºä¾‹:
            class MyTask(AnalysisTask):
                def analyze(self, context, parsed_data):
                    # è‡ªå®šä¹‰åˆ†æé€»è¾‘
                    results = []
                    # ... åˆ†æä»£ç  ...
                    return results
            
            ai.register_task(MyTask())
        """
        self.tasks.append(task)
        print(f"âœ“ ä»»åŠ¡å·²æ³¨å†Œ: {task.get_name()}")
    
    def execute_task(self, task: Union[TaskInterface, str]) -> List[AnalysisResult]:
        """
        æ‰§è¡Œä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡å¯¹è±¡æˆ–ä»»åŠ¡åç§°
            
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        # æŸ¥æ‰¾ä»»åŠ¡
        if isinstance(task, str):
            task_obj = None
            for t in self.tasks:
                if t.get_name() == task:
                    task_obj = t
                    break
            if not task_obj:
                raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task}")
        else:
            task_obj = task
        
        # æ‰§è¡Œä»»åŠ¡
        print(f"â–¶ æ‰§è¡Œä»»åŠ¡: {task_obj.get_name()}")
        results = task_obj.execute(self)
        
        # ä¿å­˜ç»“æœ
        self.task_results[task_obj.get_name()] = results
        self.reporter.add_results(results)
        
        print(f"âœ“ ä»»åŠ¡å®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results
    
    def execute_all_tasks(self) -> Dict[str, List[AnalysisResult]]:
        """
        æ‰§è¡Œæ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡
        
        Returns:
            ä»»åŠ¡åç§°åˆ°ç»“æœåˆ—è¡¨çš„å­—å…¸
        """
        print(f"â–¶ æ‰§è¡Œ {len(self.tasks)} ä¸ªä»»åŠ¡...")
        
        for task in self.tasks:
            self.execute_task(task)
        
        return self.task_results
    
    def list_tasks(self) -> List[str]:
        """
        åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡
        
        Returns:
            ä»»åŠ¡åç§°åˆ—è¡¨
        """
        return [task.get_name() for task in self.tasks]
    
    # ========== æ—¥å¿—åˆ†æ API ==========
    
    def load_log_file(self, file_path: str):
        """
        åŠ è½½æ—¥å¿—æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        self.log_file_path = file_path
        print(f"âœ“ å·²åŠ è½½æ—¥å¿—æ–‡ä»¶: {file_path}")
    
    def load_specification(self, spec_path: str):
        """
        åŠ è½½è§„èŒƒæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            spec_path: è§„èŒƒæ–‡æ¡£è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        """
        if os.path.isdir(spec_path):
            self.load_documents(spec_path)
        elif os.path.isfile(spec_path):
            with open(spec_path, 'r', encoding='utf-8') as f:
                content = f.read()
            self.add_text(content)
        else:
            raise ValueError(f"è§„èŒƒæ–‡æ¡£ä¸å­˜åœ¨: {spec_path}")
        
        print(f"âœ“ è§„èŒƒæ–‡æ¡£å·²åŠ è½½åˆ°çŸ¥è¯†åº“")
    
    def query_specification(self, query: str, top_k: int = 5) -> str:
        """
        æŸ¥è¯¢è§„èŒƒæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ç›¸å…³æ–‡æ¡£å†…å®¹
        """
        context = self.vector_db.search(query, top_k=top_k)
        return context
    
    # ========== æŠ¥å‘Šç”Ÿæˆ API ==========
    
    def generate_report(self, format: str = "markdown") -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            format: æŠ¥å‘Šæ ¼å¼ï¼Œæ”¯æŒ "text", "markdown", "json"
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        if format == "text":
            return self.reporter.generate_summary()
        elif format == "markdown":
            return self.reporter.generate_markdown_report()
        elif format == "json":
            return self.reporter.generate_json_report()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼: {format}")
    
    def save_report(self, file_path: str, format: str = "markdown"):
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            format: æŠ¥å‘Šæ ¼å¼
        """
        report = self.generate_report(format)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {file_path}")
    
    # ========== AI è¾…åŠ©åˆ†æ API ==========
    
    def analyze_with_llm(self, prompt: str, use_rag: bool = True) -> str:
        """
        ä½¿ç”¨ LLM è¿›è¡Œè¾…åŠ©åˆ†æ
        
        Args:
            prompt: åˆ†ææç¤º
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æŸ¥è¯¢è§„èŒƒæ–‡æ¡£
            
        Returns:
            LLM çš„åˆ†æç»“æœ
        """
        return self.chat(prompt, use_rag=use_rag)
    
    def ask_about_log(self, question: str) -> str:
        """
        è¯¢é—®å…³äºæ—¥å¿—çš„é—®é¢˜
        
        Args:
            question: é—®é¢˜
            
        Returns:
            LLM çš„å›ç­”
        """
        if self.log_file_path:
            # è¯»å–æ—¥å¿—å†…å®¹
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                log_content = f.read(10000)  # è¯»å–å‰ 10000 å­—ç¬¦
            
            prompt = f"""åŸºäºä»¥ä¸‹æ—¥å¿—å†…å®¹å›ç­”é—®é¢˜ï¼š

æ—¥å¿—å†…å®¹ï¼ˆéƒ¨åˆ†ï¼‰ï¼š
```
{log_content}
```

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
            return self.llm.invoke(prompt)
        else:
            return self.chat(question, use_rag=True)


__all__ = ["BitwiseAI"]
