# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ç®¡ç†æ¨¡å—

è´Ÿè´£æ–‡æ¡£çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šåŠ è½½ã€åˆ‡åˆ†ã€å»é‡ã€å­˜å‚¨ã€å¯¼å‡º
"""

import os
import json
import time
import re
from typing import List, Dict, Optional, Any

from ..utils import DocumentLoader, TextSplitter


class DocumentManager:
    """
    æ–‡æ¡£ç®¡ç†æ¨¡å—

    è´Ÿè´£æ–‡æ¡£çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
    """

    def __init__(
        self,
        memory_manager,
        document_loader: Optional[DocumentLoader] = None,
        text_splitter: Optional[TextSplitter] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç®¡ç†å™¨

        Args:
            memory_manager: MemoryManager å®ä¾‹
            document_loader: æ–‡æ¡£åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            text_splitter: æ–‡æœ¬åˆ‡åˆ†å™¨ï¼ˆå¯é€‰ï¼‰
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼‰
                - save_chunks: æ˜¯å¦ä¿å­˜åˆ‡åˆ†ç»“æœï¼ˆé»˜è®¤Falseï¼‰
                - chunks_dir: åˆ‡åˆ†ç»“æœä¿å­˜ç›®å½•
        """
        from ..core.memory import MemoryManager

        if not isinstance(memory_manager, MemoryManager):
            raise TypeError("memory_manager must be an instance of MemoryManager")

        self.memory_manager = memory_manager
        self.document_loader = document_loader or DocumentLoader()
        self.text_splitter = text_splitter or TextSplitter()
        self.config = config or {}

        # é»˜è®¤é…ç½®
        self.similarity_threshold = self.config.get("similarity_threshold", 0.85)
        self.save_chunks = self.config.get("save_chunks", False)
        self.chunks_dir = self.config.get("chunks_dir", "~/.bitwiseai/chunks")

    def load_documents(
        self,
        folder_path: str,
        skip_duplicates: bool = True
    ) -> Dict[str, Any]:
        """
        åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ–‡æ¡£

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼š
                - total: æ€»æ–‡æ¡£ç‰‡æ®µæ•°
                - inserted: å®é™…æ’å…¥çš„ç‰‡æ®µæ•°
                - skipped: è·³è¿‡çš„é‡å¤ç‰‡æ®µæ•°
        """
        if not folder_path:
            return {"total": 0, "inserted": 0, "skipped": 0}

        # 1. åŠ è½½æ–‡æ¡£
        documents = self.document_loader.load_folder(folder_path)

        if not documents:
            return {"total": 0, "inserted": 0, "skipped": 0}

        # 2. åˆ‡åˆ†æ–‡æ¡£
        chunks_with_metadata = []
        for doc in documents:
            chunks = self.text_splitter.split(doc["content"])

            # æå–æ–‡æ¡£åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
            file_path = doc["file_path"]
            file_name = os.path.splitext(os.path.basename(file_path))[0]

            # æå–æ–‡æ¡£åå…³é”®è¯
            file_name_keywords = self._extract_filename_keywords(file_name)

            for idx, chunk in enumerate(chunks):
                chunks_with_metadata.append({
                    "text": chunk,
                    "source_file": file_path,
                    "file_name": file_name,
                    "file_name_keywords": file_name_keywords,
                    "file_hash": doc["file_hash"],
                    "chunk_index": idx,
                    "chunk_total": len(chunks),
                    "timestamp": doc["timestamp"],
                    "text_length": len(chunk)
                })

        total_chunks = len(chunks_with_metadata)

        # 3. å»é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if skip_duplicates and total_chunks > 0:
            chunks_with_metadata = self._deduplicate_chunks(chunks_with_metadata)

        skipped_count = total_chunks - len(chunks_with_metadata)

        # 4. å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
        inserted_count = 0
        if chunks_with_metadata:
            print(f"ğŸ“š å¼€å§‹å¤„ç† {len(chunks_with_metadata)} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")

            for chunk_data in chunks_with_metadata:
                # ä½¿ç”¨ MemoryManager ç´¢å¼•æ–‡æ¡£
                result = self.memory_manager.index_document(
                    doc_path=chunk_data["source_file"],
                    content=chunk_data["text"]
                )
                if result.success:
                    inserted_count += 1

            print(f"âœ… æˆåŠŸæ’å…¥ {inserted_count} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°è®°å¿†ç³»ç»Ÿ")

        # 5. å¯é€‰ï¼šä¿å­˜åˆ‡åˆ†ç»“æœ
        if self.save_chunks and chunks_with_metadata:
            self._save_chunks(chunks_with_metadata)

        return {
            "total": total_chunks,
            "inserted": inserted_count,
            "skipped": skipped_count
        }

    def add_text(
        self,
        text: str,
        source: Optional[str] = None,
        skip_duplicates: bool = True
    ) -> int:
        """
        æ·»åŠ å•ä¸ªæ–‡æœ¬åˆ°è®°å¿†ç³»ç»Ÿ

        Args:
            text: æ–‡æœ¬å†…å®¹
            source: æºæ ‡è¯†ï¼ˆå¯é€‰ï¼‰
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤

        Returns:
            æ’å…¥çš„ç‰‡æ®µæ•°é‡
        """
        if not text or not text.strip():
            return 0

        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self.text_splitter.split(text)

        if not chunks:
            return 0

        # å‡†å¤‡å…ƒæ•°æ®
        current_time = time.time()

        # æå–æ–‡æ¡£åå’Œå…³é”®è¯
        if source:
            file_name = os.path.splitext(os.path.basename(source))[0]
            file_name_keywords = self._extract_filename_keywords(file_name)
        else:
            file_name = ""
            file_name_keywords = ""

        inserted_count = 0

        for idx, chunk in enumerate(chunks):
            chunk_data = {
                "text": chunk,
                "source_file": source or "",
                "file_name": file_name,
                "file_name_keywords": file_name_keywords,
                "file_hash": "",
                "chunk_index": idx,
                "chunk_total": len(chunks),
                "timestamp": current_time,
                "text_length": len(chunk)
            }

            # ä½¿ç”¨ MemoryManager ç´¢å¼•
            result = self.memory_manager.index_document(
                doc_path=source or f"text_{int(time.time())}_{idx}",
                content=chunk
            )
            if result.success:
                inserted_count += 1

        return inserted_count

    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åŸºäºæœç´¢ç›¸ä¼¼åº¦å»é‡

        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨

        Returns:
            å»é‡åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not chunks:
            return []

        # ä½¿ç”¨ MemoryManager çš„æœç´¢åŠŸèƒ½æ£€æŸ¥é‡å¤
        unique_chunks = []

        for chunk in chunks:
            # æœç´¢ç›¸ä¼¼å†…å®¹
            results = self.memory_manager.search_sync(
                query=chunk["text"][:200],  # æœç´¢å‰200å­—ç¬¦
                max_results=1
            )

            # å¦‚æœæ‰¾åˆ°é«˜ç›¸ä¼¼åº¦ç»“æœï¼Œåˆ™è·³è¿‡
            if results and results[0].score >= self.similarity_threshold:
                continue

            unique_chunks.append(chunk)

        return unique_chunks

    def check_duplicates(self, texts: List[str]) -> List[bool]:
        """
        æ£€æŸ¥æ–‡æœ¬åˆ—è¡¨ä¸­çš„é‡å¤é¡¹

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            å¸ƒå°”åˆ—è¡¨ï¼ŒTrueè¡¨ç¤ºé‡å¤
        """
        if not texts:
            return []

        is_duplicate = []

        for text in texts:
            # æœç´¢ç›¸ä¼¼å†…å®¹
            results = self.memory_manager.search_sync(
                query=text[:200],
                max_results=1
            )

            # å¦‚æœæ‰¾åˆ°é«˜ç›¸ä¼¼åº¦ç»“æœï¼Œåˆ™æ ‡è®°ä¸ºé‡å¤
            if results and results[0].score >= self.similarity_threshold:
                is_duplicate.append(True)
            else:
                is_duplicate.append(False)

        return is_duplicate

    def export_documents(
        self,
        output_dir: str,
        format: str = "separate_md"
    ) -> int:
        """
        ä»è®°å¿†ç³»ç»Ÿå¯¼å‡ºæ–‡æ¡£ä¸ºMDæ ¼å¼

        Args:
            output_dir: è¾“å‡ºç›®å½•
            format: å¯¼å‡ºæ ¼å¼ï¼ˆ"separate_md": æŒ‰æºæ–‡ä»¶åˆ†åˆ«å¯¼å‡ºï¼‰

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶æ•°é‡
        """
        if format != "separate_md":
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.expanduser(output_dir)
        os.makedirs(output_dir, exist_ok=True)

        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£æº
            from ..core.memory import MemorySource
            results = self.memory_manager.search_sync(
                query="*",
                max_results=10000,
                source_filter=[MemorySource.DOCS]
            )

            if not results:
                return 0

            # æŒ‰æºæ–‡ä»¶åˆ†ç»„
            files_dict = {}
            for item in results:
                source_file = item.path
                if source_file not in files_dict:
                    files_dict[source_file] = []

                files_dict[source_file].append({
                    "text": item.text,
                    "chunk_id": item.chunk_id
                })

            # æŒ‰æºæ–‡ä»¶å¯¼å‡º
            exported_count = 0
            for source_file, chunks in files_dict.items():
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                if source_file and source_file != "unknown":
                    base_name = os.path.basename(source_file)
                    if not base_name.endswith(".md"):
                        base_name = base_name.rsplit(".", 1)[0] + ".md"
                else:
                    base_name = f"document_{exported_count + 1}.md"

                output_path = os.path.join(output_dir, base_name)

                # åˆå¹¶chunkså¹¶å†™å…¥æ–‡ä»¶
                content = "\n\n".join([chunk["text"] for chunk in chunks])
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(content)

                exported_count += 1

            return exported_count
        except Exception as e:
            print(f"âš ï¸  å¯¼å‡ºæ–‡æ¡£å¤±è´¥: {e}")
            return 0

    def get_document_stats(self) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            stats = self.memory_manager.stats()
            return {
                "total_chunks": stats.total_chunks,
                "total_files": stats.total_files,
                "db_size_bytes": stats.db_size_bytes
            }
        except Exception as e:
            print(f"âš ï¸  è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_chunks": 0, "total_files": 0, "db_size_bytes": 0}

    def _extract_filename_keywords(self, file_name: str) -> str:
        """
        æå–æ–‡æ¡£åå…³é”®è¯

        Args:
            file_name: æ–‡æ¡£åï¼ˆä¸å«æ‰©å±•åï¼‰

        Returns:
            å…³é”®è¯å­—ç¬¦ä¸²ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰
        """
        if not file_name:
            return ""

        # å°è¯•ä½¿ç”¨jiebaåˆ†è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import jieba
            words = jieba.cut(file_name)
            stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å¦‚æœ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›'}
            keywords = [w for w in words if w not in stop_words and len(w) > 1]
            return " ".join(keywords)
        except ImportError:
            text = re.sub(r'[_\-\s]+', ' ', file_name)
            text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
            words = [w for w in text.split() if len(w) > 1]
            return " ".join(words)

    def _save_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """
        ä¿å­˜åˆ‡åˆ†ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not chunks:
            return

        chunks_dir = os.path.expanduser(self.chunks_dir)
        os.makedirs(chunks_dir, exist_ok=True)

        # æŒ‰æºæ–‡ä»¶åˆ†ç»„ä¿å­˜
        files_dict = {}
        for chunk in chunks:
            source_file = chunk.get("source_file", "unknown")
            if source_file not in files_dict:
                files_dict[source_file] = []

            files_dict[source_file].append(chunk)

        # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„chunks
        for source_file, file_chunks in files_dict.items():
            if source_file and source_file != "unknown":
                base_name = os.path.basename(source_file)
                json_name = base_name.rsplit(".", 1)[0] + "_chunks.json"
            else:
                json_name = f"chunks_{int(time.time())}.json"

            json_path = os.path.join(chunks_dir, json_name)

            try:
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(file_chunks, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜åˆ‡åˆ†ç»“æœå¤±è´¥ {json_path}: {e}")


__all__ = ["DocumentManager"]
