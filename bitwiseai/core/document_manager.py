# -*- coding: utf-8 -*-
"""
æ–‡æ¡£ç®¡ç†æ¨¡å—

è´Ÿè´£æ–‡æ¡£çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šåŠ è½½ã€åˆ‡åˆ†ã€å»é‡ã€å­˜å‚¨ã€å¯¼å‡º
"""
import os
import json
import time
import re
from typing import List, Dict, Optional, Any
from ..vector_database import MilvusDB
from ..utils import DocumentLoader, TextSplitter


class DocumentManager:
    """
    æ–‡æ¡£ç®¡ç†æ¨¡å—

    è´Ÿè´£æ–‡æ¡£çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
    """

    def __init__(
        self,
        vector_db: MilvusDB,
        document_loader: Optional[DocumentLoader] = None,
        text_splitter: Optional[TextSplitter] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç®¡ç†å™¨

        Args:
            vector_db: å‘é‡æ•°æ®åº“å®ä¾‹
            document_loader: æ–‡æ¡£åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            text_splitter: æ–‡æœ¬åˆ‡åˆ†å™¨ï¼ˆå¯é€‰ï¼‰
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼‰
                - save_chunks: æ˜¯å¦ä¿å­˜åˆ‡åˆ†ç»“æœï¼ˆé»˜è®¤Falseï¼‰
                - chunks_dir: åˆ‡åˆ†ç»“æœä¿å­˜ç›®å½•
        """
        self.vector_db = vector_db
        self.document_loader = document_loader or DocumentLoader()
        self.text_splitter = text_splitter or TextSplitter()
        self.config = config or {}
        
        # é»˜è®¤é…ç½®
        self.similarity_threshold = self.config.get("similarity_threshold", 0.85)
        self.save_chunks = self.config.get("save_chunks", False)
        self.chunks_dir = self.config.get("chunks_dir", "~/.bitwiseai/chunks")

    def load_documents(
        self,
        folder_path: str,
        skip_duplicates: bool = True
    ) -> Dict[str, Any]:
        """
        åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ–‡æ¡£

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ï¼š
                - total: æ€»æ–‡æ¡£ç‰‡æ®µæ•°
                - inserted: å®é™…æ’å…¥çš„ç‰‡æ®µæ•°
                - skipped: è·³è¿‡çš„é‡å¤ç‰‡æ®µæ•°
        """
        if not folder_path:
            return {"total": 0, "inserted": 0, "skipped": 0}

        # 1. åŠ è½½æ–‡æ¡£
        documents = self.document_loader.load_folder(folder_path)

        if not documents:
            return {"total": 0, "inserted": 0, "skipped": 0}

        # 2. åˆ‡åˆ†æ–‡æ¡£
        chunks_with_metadata = []
        for doc in documents:
            chunks = self.text_splitter.split(doc["content"])
            
            # æå–æ–‡æ¡£åï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
            file_path = doc["file_path"]
            file_name = os.path.splitext(os.path.basename(file_path))[0]  # å»æ‰æ‰©å±•å
            
            # æå–æ–‡æ¡£åå…³é”®è¯
            file_name_keywords = self._extract_filename_keywords(file_name)
            
            for idx, chunk in enumerate(chunks):
                chunks_with_metadata.append({
                    "text": chunk,
                    "source_file": file_path,
                    "file_name": file_name,  # æ–°å¢ï¼šæ–‡æ¡£å
                    "file_name_keywords": file_name_keywords,  # æ–°å¢ï¼šæ–‡æ¡£åå…³é”®è¯
                    "file_hash": doc["file_hash"],
                    "chunk_index": idx,
                    "chunk_total": len(chunks),
                    "timestamp": doc["timestamp"],
                    "text_length": len(chunk)
                })

        total_chunks = len(chunks_with_metadata)

        # 3. å»é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if skip_duplicates and total_chunks > 0:
            chunks_with_metadata = self._deduplicate_chunks(chunks_with_metadata)

        skipped_count = total_chunks - len(chunks_with_metadata)

        # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        inserted_count = 0
        if chunks_with_metadata:
            texts = [c["text"] for c in chunks_with_metadata]
            metadata = [
                {
                    "source_file": c["source_file"],
                    "file_name": c.get("file_name", ""),  # æ–°å¢
                    "file_name_keywords": c.get("file_name_keywords", ""),  # æ–°å¢
                    "file_hash": c["file_hash"],
                    "chunk_index": c["chunk_index"],
                    "chunk_total": c["chunk_total"],
                    "timestamp": c["timestamp"],
                    "text_length": c["text_length"]
                }
                for c in chunks_with_metadata
            ]
            
            # æ·»åŠ è¿›åº¦æ˜¾ç¤º
            print(f"ğŸ“š å¼€å§‹å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ...")
            try:
                inserted_count = self.vector_db.add_texts_with_metadata(texts, metadata)
                print(f"âœ… æˆåŠŸæ’å…¥ {inserted_count} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“")
            except Exception as e:
                print(f"âŒ æ’å…¥æ–‡æ¡£å¤±è´¥: {e}")
                raise

        # 5. å¯é€‰ï¼šä¿å­˜åˆ‡åˆ†ç»“æœ
        if self.save_chunks and chunks_with_metadata:
            self._save_chunks(chunks_with_metadata)

        return {
            "total": total_chunks,
            "inserted": inserted_count,
            "skipped": skipped_count
        }

    def add_text(
        self,
        text: str,
        source: Optional[str] = None,
        skip_duplicates: bool = True
    ) -> int:
        """
        æ·»åŠ å•ä¸ªæ–‡æœ¬åˆ°å‘é‡æ•°æ®åº“

        Args:
            text: æ–‡æœ¬å†…å®¹
            source: æºæ ‡è¯†ï¼ˆå¯é€‰ï¼‰
            skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤

        Returns:
            æ’å…¥çš„ç‰‡æ®µæ•°é‡
        """
        if not text or not text.strip():
            return 0

        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self.text_splitter.split(text)

        if not chunks:
            return 0

        # å‡†å¤‡å…ƒæ•°æ®
        chunks_with_metadata = []
        current_time = time.time()
        
        # æå–æ–‡æ¡£åå’Œå…³é”®è¯
        if source:
            file_name = os.path.splitext(os.path.basename(source))[0]
            file_name_keywords = self._extract_filename_keywords(file_name)
        else:
            file_name = ""
            file_name_keywords = ""
        
        for idx, chunk in enumerate(chunks):
            chunks_with_metadata.append({
                "text": chunk,
                "source_file": source or "",
                "file_name": file_name,  # æ–°å¢
                "file_name_keywords": file_name_keywords,  # æ–°å¢
                "file_hash": "",
                "chunk_index": idx,
                "chunk_total": len(chunks),
                "timestamp": current_time,
                "text_length": len(chunk)
            })

        # å»é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if skip_duplicates:
            chunks_with_metadata = self._deduplicate_chunks(chunks_with_metadata)

        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if chunks_with_metadata:
            texts = [c["text"] for c in chunks_with_metadata]
            metadata = [
                {
                    "source_file": c["source_file"],
                    "file_name": c.get("file_name", ""),  # æ–°å¢
                    "file_name_keywords": c.get("file_name_keywords", ""),  # æ–°å¢
                    "file_hash": c["file_hash"],
                    "chunk_index": c["chunk_index"],
                    "chunk_total": c["chunk_total"],
                    "timestamp": c["timestamp"],
                    "text_length": c["text_length"]
                }
                for c in chunks_with_metadata
            ]
            return self.vector_db.add_texts_with_metadata(texts, metadata)

        return 0

    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åŸºäºåµŒå…¥å‘é‡ç›¸ä¼¼åº¦å»é‡

        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨

        Returns:
            å»é‡åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not chunks:
            return []

        # ç”ŸæˆåµŒå…¥å‘é‡
        texts = [c["text"] for c in chunks]
        try:
            print(f"ğŸ”„ æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡è¿›è¡Œå»é‡æ£€æŸ¥ ({len(texts)} ä¸ªæ–‡æœ¬)...")
            vectors = self.vector_db.embedding_model.embed_documents(texts)
        except Exception as e:
            print(f"âš ï¸  ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            return chunks  # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ—è¡¨

        # åœ¨Milvusä¸­æœç´¢ç›¸ä¼¼å‘é‡
        threshold = self.similarity_threshold
        try:
            similar_results = self.vector_db.search_similar_vectors(
                vectors, threshold, top_k=1
            )

            # è¿‡æ»¤é‡å¤çš„chunks
            unique_chunks = []
            for i, chunk in enumerate(chunks):
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼ç»“æœï¼Œæˆ–è€…ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œåˆ™ä¿ç•™
                if not similar_results[i] or len(similar_results[i]) == 0:
                    unique_chunks.append(chunk)
                else:
                    # æ£€æŸ¥æœ€é«˜ç›¸ä¼¼åº¦æ˜¯å¦ä½äºé˜ˆå€¼
                    max_similarity = max([r["score"] for r in similar_results[i]], default=0.0)
                    if max_similarity < threshold:
                        unique_chunks.append(chunk)
                    # å¦åˆ™è·³è¿‡ï¼ˆé‡å¤ï¼‰

            return unique_chunks
        except Exception as e:
            print(f"âš ï¸  å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return chunks  # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ—è¡¨

    def check_duplicates(self, texts: List[str]) -> List[bool]:
        """
        æ£€æŸ¥æ–‡æœ¬åˆ—è¡¨ä¸­çš„é‡å¤é¡¹

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            å¸ƒå°”åˆ—è¡¨ï¼ŒTrueè¡¨ç¤ºé‡å¤
        """
        if not texts:
            return []

        # ç”ŸæˆåµŒå…¥å‘é‡
        try:
            print(f"ğŸ”„ æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡è¿›è¡Œé‡å¤æ£€æŸ¥ ({len(texts)} ä¸ªæ–‡æœ¬)...")
            vectors = self.vector_db.embedding_model.embed_documents(texts)
        except Exception as e:
            print(f"âš ï¸  ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            return [False] * len(texts)

        # æœç´¢ç›¸ä¼¼å‘é‡
        threshold = self.similarity_threshold
        try:
            similar_results = self.vector_db.search_similar_vectors(
                vectors, threshold, top_k=1
            )

            # è¿”å›æ˜¯å¦é‡å¤
            is_duplicate = []
            for results in similar_results:
                if results and len(results) > 0:
                    max_similarity = max([r["score"] for r in results], default=0.0)
                    is_duplicate.append(max_similarity >= threshold)
                else:
                    is_duplicate.append(False)

            return is_duplicate
        except Exception as e:
            print(f"âš ï¸  é‡å¤æ£€æŸ¥å¤±è´¥: {e}")
            return [False] * len(texts)

    def export_documents(
        self,
        output_dir: str,
        format: str = "separate_md"
    ) -> int:
        """
        ä»å‘é‡æ•°æ®åº“å¯¼å‡ºæ–‡æ¡£ä¸ºMDæ ¼å¼

        Args:
            output_dir: è¾“å‡ºç›®å½•
            format: å¯¼å‡ºæ ¼å¼ï¼ˆ"separate_md": æŒ‰æºæ–‡ä»¶åˆ†åˆ«å¯¼å‡ºï¼‰

        Returns:
            å¯¼å‡ºçš„æ–‡ä»¶æ•°é‡
        """
        if format != "separate_md":
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.expanduser(output_dir)
        os.makedirs(output_dir, exist_ok=True)

        # ä»å‘é‡æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£
        try:
            # æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜ï¼‰
            query_result = self.vector_db.client.query(
                collection_name=self.vector_db.collection_name,
                filter="",
                limit=10000,  # é™åˆ¶æŸ¥è¯¢æ•°é‡
                output_fields=["text", "source_file", "file_hash", "chunk_index", "chunk_total"]
            )

            if not query_result:
                return 0

            # æŒ‰æºæ–‡ä»¶åˆ†ç»„
            files_dict = {}
            for item in query_result:
                source_file = item.get("source_file", "unknown")
                if source_file not in files_dict:
                    files_dict[source_file] = []

                files_dict[source_file].append({
                    "text": item.get("text", ""),
                    "chunk_index": item.get("chunk_index", 0),
                    "chunk_total": item.get("chunk_total", 1)
                })

            # æŒ‰æºæ–‡ä»¶å¯¼å‡º
            exported_count = 0
            for source_file, chunks in files_dict.items():
                # æŒ‰chunk_indexæ’åº
                chunks.sort(key=lambda x: x["chunk_index"])

                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                if source_file and source_file != "unknown":
                    # ä½¿ç”¨æºæ–‡ä»¶å
                    base_name = os.path.basename(source_file)
                    if not base_name.endswith(".md"):
                        base_name = base_name.rsplit(".", 1)[0] + ".md"
                else:
                    base_name = f"document_{exported_count + 1}.md"

                output_path = os.path.join(output_dir, base_name)

                # åˆå¹¶chunkså¹¶å†™å…¥æ–‡ä»¶
                content = "\n\n".join([chunk["text"] for chunk in chunks])
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(content)

                exported_count += 1

            return exported_count
        except Exception as e:
            print(f"âš ï¸  å¯¼å‡ºæ–‡æ¡£å¤±è´¥: {e}")
            return 0

    def get_document_stats(self) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            count = self.vector_db.count()
            return {
                "total_chunks": count,
                "collection_name": self.vector_db.collection_name,
                "db_file": self.vector_db.db_file
            }
        except Exception as e:
            print(f"âš ï¸  è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_chunks": 0, "collection_name": "", "db_file": ""}

    def _extract_filename_keywords(self, file_name: str) -> str:
        """
        æå–æ–‡æ¡£åå…³é”®è¯
        
        Args:
            file_name: æ–‡æ¡£åï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            å…³é”®è¯å­—ç¬¦ä¸²ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰
        """
        if not file_name:
            return ""
        
        # å°è¯•ä½¿ç”¨jiebaåˆ†è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import jieba
            # åˆ†è¯
            words = jieba.cut(file_name)
            # è¿‡æ»¤åœç”¨è¯å’Œå•å­—ç¬¦
            stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å¦‚æœ', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªäº›'}
            keywords = [w for w in words if w not in stop_words and len(w) > 1]
            return " ".join(keywords)
        except ImportError:
            # å¦‚æœæ²¡æœ‰jiebaï¼Œä½¿ç”¨ç®€å•æ–¹æ³•ï¼šæŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
            # å¤„ç†ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ã€é©¼å³°å‘½åç­‰
            # æ›¿æ¢å¸¸è§åˆ†éš”ç¬¦ä¸ºç©ºæ ¼
            text = re.sub(r'[_\-\s]+', ' ', file_name)
            # å¤„ç†é©¼å³°å‘½åï¼ˆç®€å•æ–¹æ³•ï¼šåœ¨å¤§å†™å­—æ¯å‰åŠ ç©ºæ ¼ï¼‰
            text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
            # åˆ†å‰²å¹¶è¿‡æ»¤
            words = [w for w in text.split() if len(w) > 1]
            return " ".join(words)
    
    def _save_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """
        ä¿å­˜åˆ‡åˆ†ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

        Args:
            chunks: æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not chunks:
            return

        chunks_dir = os.path.expanduser(self.chunks_dir)
        os.makedirs(chunks_dir, exist_ok=True)

        # æŒ‰æºæ–‡ä»¶åˆ†ç»„ä¿å­˜
        files_dict = {}
        for chunk in chunks:
            source_file = chunk.get("source_file", "unknown")
            if source_file not in files_dict:
                files_dict[source_file] = []

            files_dict[source_file].append(chunk)

        # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„chunks
        for source_file, file_chunks in files_dict.items():
            # ç”Ÿæˆæ–‡ä»¶å
            if source_file and source_file != "unknown":
                base_name = os.path.basename(source_file)
                json_name = base_name.rsplit(".", 1)[0] + "_chunks.json"
            else:
                json_name = f"chunks_{int(time.time())}.json"

            json_path = os.path.join(chunks_dir, json_name)

            # ä¿å­˜ä¸ºJSON
            try:
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(file_chunks, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜åˆ‡åˆ†ç»“æœå¤±è´¥ {json_path}: {e}")


__all__ = ["DocumentManager"]

