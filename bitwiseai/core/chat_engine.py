# -*- coding: utf-8 -*-
"""
èŠå¤©å¼•æ“

æ•´åˆ RAGã€Skillsã€Slash å‘½ä»¤å’Œ Flowï¼Œæ”¯æŒ LangChain Agent å’Œæµå¼è¾“å‡º
"""
from typing import Iterator, Optional, List
from ..llm import LLM
from .rag_engine import RAGEngine
from .skill_manager import SkillManager
from .slash import SlashCommandRegistry, parse_slash_command_call
from .flow import create_ralph_flow, FlowRunner
from .flow.ralph import RalphLoopConfig


class ChatEngine:
    """
    èŠå¤©å¼•æ“

    æ•´åˆ RAGã€Skillsã€Slash å‘½ä»¤å’Œ Flowï¼Œæä¾›ç»Ÿä¸€çš„èŠå¤©æ¥å£
    """

    def __init__(
        self,
        llm: LLM,
        rag_engine: Optional[RAGEngine] = None,
        skill_manager: Optional[SkillManager] = None,
        system_prompt: str = "",
        enable_slash: bool = True,
        enable_ralph_loop: bool = True,
        ralph_max_iterations: int = 10,
    ):
        """
        åˆå§‹åŒ–èŠå¤©å¼•æ“

        Args:
            llm: LLM å®ä¾‹
            rag_engine: RAG å¼•æ“ï¼ˆå¯é€‰ï¼‰
            skill_manager: Skill ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            enable_slash: æ˜¯å¦å¯ç”¨ Slash å‘½ä»¤
            enable_ralph_loop: æ˜¯å¦å¯ç”¨ Ralph Loop è‡ªåŠ¨è¿­ä»£
            ralph_max_iterations: Ralph Loop é»˜è®¤æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        self.llm = llm
        self.rag_engine = rag_engine
        self.skill_manager = skill_manager
        self.system_prompt = system_prompt

        # Slash å‘½ä»¤ç³»ç»Ÿ
        self.enable_slash = enable_slash
        self._slash_registry = SlashCommandRegistry()
        self._setup_slash_commands()

        # Ralph Loop é…ç½®
        self.enable_ralph_loop = enable_ralph_loop
        self.ralph_max_iterations = ralph_max_iterations
        self.ralph_config = RalphLoopConfig(max_iterations=ralph_max_iterations)

        # å†å²æ¶ˆæ¯ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        self.history: List[dict] = []

        # YOLO æ¨¡å¼ï¼ˆè‡ªåŠ¨å®¡æ‰¹ï¼‰
        self.yolo_mode = False

    def _setup_slash_commands(self) -> None:
        """è®¾ç½®æ‰€æœ‰ Slash å‘½ä»¤"""
        if not self.enable_slash:
            return

        from .slash.commands import register_all_commands
        register_all_commands(self._slash_registry)

    def list_slash_commands(self) -> List[str]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ Slash å‘½ä»¤

        Returns:
            å‘½ä»¤åç§°åˆ—è¡¨
        """
        return self._slash_registry.list_names()

    def get_slash_command_help(self, command_name: str) -> str | None:
        """
        è·å– Slash å‘½ä»¤çš„å¸®åŠ©ä¿¡æ¯

        Args:
            command_name: å‘½ä»¤åç§°

        Returns:
            å‘½ä»¤æè¿°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        cmd = self._slash_registry.get(command_name)
        return cmd.description if cmd else None

    async def _handle_slash_command(self, query: str) -> str | None:
        """
        å¤„ç† Slash å‘½ä»¤

        æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. /command - å†…ç½®å‘½ä»¤ï¼ˆå¦‚ /help, /clearï¼‰
        2. /skill-name - æŠ€èƒ½åç§°ï¼ˆè‡ªåŠ¨åŠ è½½å¹¶ä½¿ç”¨æŠ€èƒ½ä¸Šä¸‹æ–‡ï¼‰

        Args:
            query: ç”¨æˆ·è¾“å…¥

        Returns:
            å‘½ä»¤æ‰§è¡Œç»“æœï¼Œå¦‚æœä¸æ˜¯å‘½ä»¤åˆ™è¿”å› None
        """
        if not self.enable_slash:
            return None

        call = parse_slash_command_call(query)
        if call is None:
            return None

        cmd = self._slash_registry.find(call)
        if cmd is None:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŠ€èƒ½åç§°
            if self.skill_manager and call.name in self.skill_manager.list_available_skills():
                # è‡ªåŠ¨åŠ è½½æŠ€èƒ½
                skill = self.skill_manager.get_skill(call.name)
                if skill and not skill.loaded:
                    self.skill_manager.load_skill(call.name)

                # è·å–æŠ€èƒ½å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
                skill = self.skill_manager.get_skill(call.name)
                if skill and skill.content:
                    # ä½¿ç”¨æŠ€èƒ½ä¸Šä¸‹æ–‡è¿›è¡Œå¯¹è¯
                    actual_query = call.args if call.args else f"ä½¿ç”¨ {call.name} æŠ€èƒ½å¸®åŠ©æˆ‘"
                    return await self._run_with_skill_context(actual_query, skill)
                else:
                    return f"æŠ€èƒ½ {call.name} å·²åŠ è½½ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°å†…å®¹ã€‚è¯·å°è¯•: {actual_query}"

            return f"æœªçŸ¥å‘½ä»¤æˆ–æŠ€èƒ½: /{call.name}\nä½¿ç”¨ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤ï¼Œæˆ– /skills æŸ¥çœ‹å¯ç”¨æŠ€èƒ½ã€‚"

        # æ‰§è¡Œå‘½ä»¤
        import inspect

        result = cmd.func(self, call.args)
        if inspect.isawaitable(result):
            result = await result

        return result

    async def _run_with_skill_context(self, query: str, skill) -> str:
        """
        ä½¿ç”¨æŠ€èƒ½ä¸Šä¸‹æ–‡è¿è¡Œå¯¹è¯

        Args:
            query: ç”¨æˆ·é—®é¢˜
            skill: æŠ€èƒ½å¯¹è±¡

        Returns:
            AI å›ç­”
        """
        # ä½¿ç”¨ RAG å’Œå·¥å…·ï¼Œä½†å¸¦æŠ€èƒ½ä¸Šä¸‹æ–‡
        return self._chat_with_tools(
            query=query,
            use_rag=True,
            history=None,
            skill_context=skill.content
        )

    async def _run_ralph_loop(self, query: str) -> str:
        """
        è¿è¡Œ Ralph Loop è‡ªåŠ¨è¿­ä»£

        Args:
            query: ç”¨æˆ·ä»»åŠ¡æè¿°
            **kwargs: ä¼ é€’ç»™ chat çš„å…¶ä»–å‚æ•°

        Returns:
            æœ€ç»ˆç»“æœ
        """
        flow = create_ralph_flow(query, self.ralph_config.max_iterations)
        runner = FlowRunner(flow, self, max_moves=self.ralph_config.max_iterations * 2)

        result = await runner.run()

        # æ„å»ºç»“æœæ¶ˆæ¯
        if result.stop_reason == "completed":
            return result.final_message or "ä»»åŠ¡å®Œæˆã€‚"
        elif result.stop_reason == "max_moves":
            return f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({self.ralph_config.max_iterations})ã€‚\n\n{result.final_message or ''}"
        else:
            return result.final_message or "æ‰§è¡Œä¸­æ–­ã€‚"

    def _convert_history_to_messages(self, history: Optional[List[dict]]) -> List:
        """
        å°†å†å²æ¶ˆæ¯è½¬æ¢ä¸º LangChain æ¶ˆæ¯æ ¼å¼
        
        Args:
            history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]
            
        Returns:
            LangChain æ¶ˆæ¯åˆ—è¡¨
        """
        from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
        
        messages = []
        
        # è½¬æ¢å†å²æ¶ˆæ¯ï¼ˆä¸åŒ…æ‹¬ç³»ç»Ÿæç¤ºè¯ï¼Œç³»ç»Ÿæç¤ºè¯ä¼šåœ¨è°ƒç”¨æ—¶å•ç‹¬æ·»åŠ ï¼‰
        if history:
            for msg in history:
                if not isinstance(msg, dict):
                    continue
                role = msg.get("role", "")
                content = msg.get("content", "")
                if not content:
                    continue
                    
                if role == "user":
                    messages.append(HumanMessage(content=content))
                elif role == "assistant":
                    messages.append(AIMessage(content=content))
                elif role == "system":
                    messages.append(SystemMessage(content=content))
        
        return messages

    def chat(
        self,
        query: str,
        use_rag: bool = True,
        use_tools: bool = True,
        history: Optional[List[dict]] = None,
        skill_context: Optional[str] = None,
        use_ralph_loop: bool = False,
    ) -> str:
        """
        èŠå¤©æ–¹æ³•ï¼ˆéæµå¼ï¼‰

        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ¨¡å¼
            use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
            history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]
            skill_context: æŠ€èƒ½ä¸Šä¸‹æ–‡å†…å®¹ï¼ˆå¯é€‰ï¼‰
            use_ralph_loop: æ˜¯å¦ä½¿ç”¨ Ralph Loop è‡ªåŠ¨è¿­ä»£

        Returns:
            LLM ç”Ÿæˆçš„å›ç­”
        """
        import asyncio

        # å¤„ç† Slash å‘½ä»¤
        slash_result = asyncio.run(self._handle_slash_command(query))
        if slash_result is not None:
            return slash_result

        # ä½¿ç”¨ Ralph Loop è‡ªåŠ¨è¿­ä»£
        if use_ralph_loop and self.enable_ralph_loop:
            return asyncio.run(self._run_ralph_loop(query, use_rag=use_rag, use_tools=use_tools, history=history, skill_context=skill_context))

        # å¦‚æœæœ‰å·¥å…·ä¸”å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨å¸¦å·¥å…·çš„èŠå¤©
        if use_tools and self.skill_manager:
            loaded_skills = self.skill_manager.list_loaded_skills()
            if len(loaded_skills) > 0 or skill_context:
                return self._chat_with_tools(query, use_rag=use_rag, history=history, skill_context=skill_context)

        if use_rag and self.rag_engine:
            return self._chat_with_rag(query, history=history, skill_context=skill_context)
        else:
            return self._chat_with_llm(query, history=history, skill_context=skill_context)

    def chat_stream(
        self,
        query: str,
        use_rag: bool = True,
        use_tools: bool = True,
        history: Optional[List[dict]] = None
    ) -> Iterator[str]:
        """
        æµå¼èŠå¤©æ–¹æ³•

        Args:
            query: ç”¨æˆ·é—®é¢˜
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ¨¡å¼
            use_tools: æ˜¯å¦ä½¿ç”¨å·¥å…·
            history: å†å²æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}, ...]

        Yields:
            æ¯ä¸ª token çš„å­—ç¬¦ä¸²ç‰‡æ®µ
        """
        # å¦‚æœæœ‰å·¥å…·ä¸”å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨å¸¦å·¥å…·çš„æµå¼èŠå¤©
        if use_tools and self.skill_manager and len(self.skill_manager.list_loaded_skills()) > 0:
            yield from self._chat_with_tools_stream(query, use_rag=use_rag, history=history)
        elif use_rag and self.rag_engine:
            yield from self._chat_with_rag_stream(query, history=history)
        else:
            yield from self._chat_with_llm_stream(query, history=history)

    def _chat_with_rag(self, query: str, history: Optional[List[dict]] = None, skill_context: Optional[str] = None) -> str:
        """
        RAG æ¨¡å¼å¯¹è¯ï¼ˆéæµå¼ï¼‰
        """
        if not self.rag_engine:
            return self._chat_with_llm(query, history=history, skill_context=skill_context)

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = self.rag_engine.search(query, top_k=5)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_parts = []
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"
        system_parts.append(base_prompt)
        
        # æ·»åŠ æŠ€èƒ½ä¸Šä¸‹æ–‡
        if skill_context:
            skills_context = "\n\n" + "=" * 60 + "\n"
            skills_context += "æŠ€èƒ½æŒ‡å¯¼å†…å®¹ï¼ˆè¯·ä¸¥æ ¼æŒ‰ç…§è¿™äº›æŒ‡å¯¼æ‰§è¡Œä»»åŠ¡ï¼‰:\n"
            skills_context += "=" * 60 + "\n\n"
            skills_context += skill_context
            skills_context += "\n\n" + "=" * 60 + "\n"
            system_parts.append(skills_context)
        
        # æ·»åŠ  RAG ä¸Šä¸‹æ–‡
        if context:
            rag_prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}"""
            system_parts.append(rag_prompt)
        
        system_content = "\n\n".join(system_parts)
        
        # æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯æˆ–æ·»åŠ æ–°çš„ç³»ç»Ÿæ¶ˆæ¯
        if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
            from langchain_core.messages import SystemMessage
            if isinstance(messages[0], SystemMessage):
                messages[0].content = system_content
            else:
                messages.insert(0, SystemMessage(content=system_content))
        else:
            from langchain_core.messages import SystemMessage
            messages.insert(0, SystemMessage(content=system_content))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        from langchain_core.messages import HumanMessage
        messages.append(HumanMessage(content=query))

        # è°ƒç”¨ LLMï¼ˆå¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼›å¦åˆ™ä½¿ç”¨å­—ç¬¦ä¸²ï¼‰
        if len(messages) > 1:
            return self.llm.invoke(messages)
        else:
            # æ²¡æœ‰å†å²ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
            prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {query}

å›ç­”:"""
            return self.llm.invoke(prompt)

    def _chat_with_rag_stream(self, query: str, history: Optional[List[dict]] = None) -> Iterator[str]:
        """
        RAG æ¨¡å¼å¯¹è¯ï¼ˆæµå¼ï¼‰
        """
        if not self.rag_engine:
            yield from self._chat_with_llm_stream(query, history=history)
            return

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = self.rag_engine.search(query, top_k=5)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        from langchain_core.messages import SystemMessage, HumanMessage
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"

        if context:
            system_content = f"""{base_prompt}

åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡:
{context}"""
        else:
            system_content = base_prompt

        # æ·»åŠ æˆ–æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯
        if messages and isinstance(messages[0], SystemMessage):
            messages[0].content = system_content
        else:
            messages.insert(0, SystemMessage(content=system_content))

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=query))

        # æµå¼è°ƒç”¨ LLM
        yield from self.llm.stream(messages)

    def _chat_with_llm(self, query: str, history: Optional[List[dict]] = None, skill_context: Optional[str] = None) -> str:
        """
        çº¯ LLM æ¨¡å¼å¯¹è¯ï¼ˆéæµå¼ï¼‰
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_parts = []
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"
        system_parts.append(base_prompt)
        
        # æ·»åŠ æŠ€èƒ½ä¸Šä¸‹æ–‡
        if skill_context:
            skills_context = "\n\n" + "=" * 60 + "\n"
            skills_context += "æŠ€èƒ½æŒ‡å¯¼å†…å®¹ï¼ˆè¯·ä¸¥æ ¼æŒ‰ç…§è¿™äº›æŒ‡å¯¼æ‰§è¡Œä»»åŠ¡ï¼‰:\n"
            skills_context += "=" * 60 + "\n\n"
            skills_context += skill_context
            skills_context += "\n\n" + "=" * 60 + "\n"
            system_parts.append(skills_context)
        
        system_content = "\n\n".join(system_parts)
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        from langchain_core.messages import SystemMessage, HumanMessage
        if messages and isinstance(messages[0], SystemMessage):
            messages[0].content = system_content
        else:
            messages.insert(0, SystemMessage(content=system_content))
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=query))
        
        # å¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼Œä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼›å¦åˆ™ä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
        if len(messages) > 1:
            return self.llm.invoke(messages)
        else:
            # æ²¡æœ‰å†å²ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ ¼å¼
            prompt = f"{system_content}\n\nç”¨æˆ·: {query}"
            return self.llm.invoke(prompt)

    def _chat_with_llm_stream(self, query: str, history: Optional[List[dict]] = None) -> Iterator[str]:
        """
        çº¯ LLM æ¨¡å¼å¯¹è¯ï¼ˆæµå¼ï¼‰
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._convert_history_to_messages(history)

        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        from langchain_core.messages import SystemMessage, HumanMessage
        if self.system_prompt:
            if messages and isinstance(messages[0], SystemMessage):
                messages[0].content = self.system_prompt
            else:
                messages.insert(0, SystemMessage(content=self.system_prompt))

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(HumanMessage(content=query))

        # æµå¼è°ƒç”¨ LLM
        yield from self.llm.stream(messages)

    def _chat_with_tools(self, query: str, use_rag: bool = True, history: Optional[List[dict]] = None, skill_context: Optional[str] = None) -> str:
        """
        ä½¿ç”¨å·¥å…·çš„å¯¹è¯æ¨¡å¼ï¼ˆéæµå¼ï¼‰

        æ”¯æŒä¸¤ç§æ–¹å¼ï¼š
        1. ç›´æ¥ Function Callingï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰ï¼šä½¿ç”¨ bind_tools
        2. ç®€åŒ–æ¨¡å¼ï¼ˆfallbackï¼‰ï¼šåœ¨ç³»ç»Ÿæç¤ºä¸­æè¿°å·¥å…·
        """
        from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage

        # è·å–å·¥å…·
        if not self.skill_manager:
            if use_rag and self.rag_engine:
                return self._chat_with_rag(query, history=history, skill_context=skill_context)
            else:
                return self._chat_with_llm(query, history=history, skill_context=skill_context)

        try:
            langchain_tools = self.skill_manager.get_tools()
        except Exception as e:
            print(f"âš ï¸  è·å–å·¥å…·å¤±è´¥: {str(e)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
            if use_rag and self.rag_engine:
                return self._chat_with_rag(query, history=history, skill_context=skill_context)
            else:
                return self._chat_with_llm(query, history=history, skill_context=skill_context)

        # å¦‚æœæ²¡æœ‰å·¥å…·ä½†æœ‰æŠ€èƒ½ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼ä½†åŒ…å«æŠ€èƒ½ä¸Šä¸‹æ–‡
        if not langchain_tools:
            if skill_context:
                # æœ‰æŠ€èƒ½ä¸Šä¸‹æ–‡ä½†æ²¡æœ‰å·¥å…·ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼ä½†åŒ…å«æŠ€èƒ½ä¸Šä¸‹æ–‡
                if use_rag and self.rag_engine:
                    return self._chat_with_rag(query, history=history, skill_context=skill_context)
                else:
                    return self._chat_with_llm(query, history=history, skill_context=skill_context)
            else:
                # æ²¡æœ‰å·¥å…·ä¹Ÿæ²¡æœ‰æŠ€èƒ½ä¸Šä¸‹æ–‡ï¼Œæ­£å¸¸å›é€€
                if use_rag and self.rag_engine:
                    return self._chat_with_rag(query, history=history)
                else:
                    return self._chat_with_llm(query, history=history)

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"

        # æ·»åŠ æŠ€èƒ½ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæä¾›ï¼‰- é™åˆ¶é•¿åº¦é¿å… API é”™è¯¯
        skills_context = ""
        if skill_context:
            # é™åˆ¶æŠ€èƒ½ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé¿å…è¶…è¿‡ API é™åˆ¶
            max_skill_length = 3000  # é™åˆ¶ä¸º 3000 å­—ç¬¦
            truncated_skill = skill_context[:max_skill_length]
            if len(skill_context) > max_skill_length:
                truncated_skill += "\n\n...(æŠ€èƒ½å†…å®¹å·²æˆªæ–­ä»¥é€‚åº” API é™åˆ¶)"

            skills_context = "\n\n" + "=" * 60 + "\n"
            skills_context += "æŠ€èƒ½æŒ‡å¯¼å†…å®¹ï¼ˆè¯·ä¸¥æ ¼æŒ‰ç…§è¿™äº›æŒ‡å¯¼æ‰§è¡Œä»»åŠ¡ï¼‰:\n"
            skills_context += "=" * 60 + "\n\n"
            skills_context += truncated_skill
            skills_context += "\n\n" + "=" * 60 + "\n"

        # å¦‚æœæœ‰ RAGï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = ""
        if use_rag and self.rag_engine:
            # ä½¿ç”¨ search_with_metadata è·å–æ›´è¯¦ç»†çš„æ£€ç´¢ç»“æœ
            results = self.rag_engine.search_with_metadata(query, top_k=5, use_hybrid=True)
            if results:
                # æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼ŒåŒ…å«æ–‡æ¡£æ¥æºä¿¡æ¯
                context_parts = []
                for i, result in enumerate(results, 1):
                    source_file = result.get('source_file', 'æœªçŸ¥')
                    text = result.get('text', '')
                    import os
                    filename = os.path.basename(source_file)
                    context_parts.append(f"[æ–‡æ¡£ {i}: {filename}]\n{text}")
                context = "\n\n---\n\n".join(context_parts)
                context = f"\n\né‡è¦ï¼šè¯·ä¼˜å…ˆä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦è¯´è‡ªå·±ä¸çŸ¥é“ã€‚\n\næ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:\n{context}\n"

        system_prompt_text = base_prompt + skills_context + (context if context else "")

        # å°è¯•ä½¿ç”¨ç›´æ¥ Function Callingï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        try:
            # ä½¿ç”¨ bind_tools ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ï¼ˆåŸç”Ÿ Function Callingï¼‰
            if hasattr(self.llm.client, 'bind_tools'):
                # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
                model_with_tools = self.llm.client.bind_tools(langchain_tools)

                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    messages.insert(0, SystemMessage(content=system_prompt_text))

                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))

                # è°ƒç”¨æ¨¡å‹ - æ•è· API é”™è¯¯
                try:
                    response = model_with_tools.invoke(messages)
                except Exception as api_error:
                    error_msg = str(api_error)
                    # å¦‚æœæ˜¯å‚æ•°é…ç½®é”™è¯¯ï¼ˆå¦‚ 2013ï¼‰ï¼Œå°è¯•ä¸ä½¿ç”¨ bind_tools
                    if '2013' in error_msg or 'invalid' in error_msg.lower() or 'params' in error_msg.lower():
                        print(f"âš ï¸  bind_tools ä¸å½“å‰ LLM ä¸å…¼å®¹ï¼Œé™çº§åˆ° Agent æ¨¡å¼")
                        raise AttributeError("bind_tools not compatible")
                    else:
                        raise

                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_messages = []
                    for tool_call in response.tool_calls:
                        tool_name = tool_call.get('name', '')
                        tool_args = tool_call.get('args', {})
                        tool_id = tool_call.get('id', '')

                        # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·
                        tool_func = None
                        for tool in langchain_tools:
                            if tool.name == tool_name:
                                tool_func = tool
                                break

                        if tool_func:
                            try:
                                # æ‰§è¡Œå·¥å…·
                                tool_result = tool_func.invoke(tool_args)
                                tool_messages.append(
                                    ToolMessage(
                                        content=str(tool_result),
                                        tool_call_id=tool_id
                                    )
                                )
                            except Exception as e:
                                tool_messages.append(
                                    ToolMessage(
                                        content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                                        tool_call_id=tool_id
                                    )
                                )
                                print(f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
                        else:
                            tool_messages.append(
                                ToolMessage(
                                    content=f"å·¥å…·ä¸å­˜åœ¨: {tool_name}",
                                    tool_call_id=tool_id
                                )
                            )

                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼Œå†æ¬¡è°ƒç”¨æ¨¡å‹
                    messages.append(response)
                    messages.extend(tool_messages)

                    # è·å–æœ€ç»ˆå›ç­”
                    final_response = model_with_tools.invoke(messages)
                    return final_response.content
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å›ç­”
                    return response.content
            else:
                # æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼
                raise AttributeError("æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼")

        except (AttributeError, Exception) as e:
            # Fallback: ä½¿ç”¨ Agent æ¨¡å¼æˆ–ç®€åŒ–æ¨¡å¼
            if "bind_tools" in str(e) or "2013" in str(e) or "compatible" in str(e):
                print(f"âš ï¸  ç›´æ¥ Function Calling ä¸å¯ç”¨ï¼Œå°è¯•ç®€åŒ–æ¨¡å¼")
            else:
                print(f"âš ï¸  Function Calling å¤±è´¥: {str(e)}ï¼Œå°è¯•ç®€åŒ–æ¨¡å¼")

            # ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥åœ¨ç³»ç»Ÿæç¤ºä¸­æè¿°å·¥å…·
            try:
                # æ„å»ºå·¥å…·æè¿°
                tools_description = "\n\nå¯ç”¨å·¥å…·:\n"
                for tool in langchain_tools:
                    tools_description += f"- {tool.name}: {tool.description}\n"

                simplified_prompt = system_prompt_text + tools_description + "\nè¯·ä½¿ç”¨ä¸Šè¿°å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚"

                # æ„å»ºæ¶ˆæ¯
                messages = self._convert_history_to_messages(history)
                if messages and isinstance(messages[0], SystemMessage):
                    messages[0].content = simplified_prompt
                else:
                    messages.insert(0, SystemMessage(content=simplified_prompt))

                messages.append(HumanMessage(content=query))

                # è°ƒç”¨ LLMï¼ˆä¸å¸¦å·¥å…·ç»‘å®šï¼‰
                response = self.llm.client.invoke(messages)

                if hasattr(response, 'content'):
                    content = response.content
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼ˆç®€å•è§£æï¼‰
                    for tool in langchain_tools:
                        tool_name = tool.name
                        if f"è°ƒç”¨{tool_name}" in content or f"ä½¿ç”¨{tool_name}" in content or f"{tool_name}(" in content:
                            # å°è¯•æå–å‚æ•°å¹¶è°ƒç”¨å·¥å…·
                            try:
                                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å‚æ•°è§£æé€»è¾‘
                                # ç›®å‰å…ˆç®€å•è¿”å›ï¼Œè®©ç”¨æˆ·æ‰‹åŠ¨è°ƒç”¨
                                print(f"\nğŸ’¡ æ£€æµ‹åˆ°å¯èƒ½éœ€è¦è°ƒç”¨å·¥å…·: {tool_name}")
                                print(f"   è¯·å°è¯•ç›´æ¥ä½¿ç”¨: /{tool.name} <å‚æ•°>")
                            except:
                                pass
                    return content
                else:
                    return str(response)

            except Exception as simple_error:
                print(f"âš ï¸  ç®€åŒ–æ¨¡å¼ä¹Ÿå¤±è´¥: {str(simple_error)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
                if use_rag and self.rag_engine:
                    return self._chat_with_rag(query, history=history, skill_context=skill_context)
                else:
                    return self._chat_with_llm(query, history=history, skill_context=skill_context)

    def _chat_with_tools_stream(self, query: str, use_rag: bool = True, history: Optional[List[dict]] = None) -> Iterator[str]:
        """
        ä½¿ç”¨å·¥å…·çš„å¯¹è¯æ¨¡å¼ï¼ˆæµå¼ï¼‰
        
        å®ç°çœŸæ­£çš„æµå¼ä¼ è¾“ï¼š
        1. å¦‚æœæ¨¡å‹æ”¯æŒ bind_toolsï¼Œå·¥å…·è°ƒç”¨åæµå¼è·å–æœ€ç»ˆå›ç­”
        2. å¯¹äº Agent æ¨¡å¼ï¼Œä½¿ç”¨æµå¼ Agent æ‰§è¡Œå™¨
        """
        from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
        from langchain.agents import create_agent

        # è·å–å·¥å…·
        if not self.skill_manager:
            if use_rag and self.rag_engine:
                yield from self._chat_with_rag_stream(query, history=history)
            else:
                yield from self._chat_with_llm_stream(query, history=history)
            return

        try:
            langchain_tools = self.skill_manager.get_tools()
        except Exception as e:
            print(f"âš ï¸  è·å–å·¥å…·å¤±è´¥: {str(e)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
            if use_rag and self.rag_engine:
                yield from self._chat_with_rag_stream(query, history=history)
            else:
                yield from self._chat_with_llm_stream(query, history=history)
            return

        if not langchain_tools:
            if use_rag and self.rag_engine:
                yield from self._chat_with_rag_stream(query, history=history)
            else:
                yield from self._chat_with_llm_stream(query, history=history)
            return

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        base_prompt = self.system_prompt or "ä½ æ˜¯ BitwiseAIï¼Œä¸“æ³¨äºç¡¬ä»¶æŒ‡ä»¤éªŒè¯å’Œè°ƒè¯•æ—¥å¿—åˆ†æçš„ AI åŠ©æ‰‹ã€‚"
        
        # å¦‚æœæœ‰ RAGï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£
        context = ""
        if use_rag and self.rag_engine:
            # ä½¿ç”¨ search_with_metadata è·å–æ›´è¯¦ç»†çš„æ£€ç´¢ç»“æœ
            results = self.rag_engine.search_with_metadata(query, top_k=5, use_hybrid=True)
            if results:
                # æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼ŒåŒ…å«æ–‡æ¡£æ¥æºä¿¡æ¯
                context_parts = []
                for i, result in enumerate(results, 1):
                    source_file = result.get('source_file', 'æœªçŸ¥')
                    text = result.get('text', '')
                    import os
                    filename = os.path.basename(source_file)
                    context_parts.append(f"[æ–‡æ¡£ {i}: {filename}]\n{text}")
                context = "\n\n---\n\n".join(context_parts)
                context = f"\n\né‡è¦ï¼šè¯·ä¼˜å…ˆä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦è¯´è‡ªå·±ä¸çŸ¥é“ã€‚\n\næ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:\n{context}\n"
        
        system_prompt_text = base_prompt + context if context else base_prompt

        # å°è¯•ä½¿ç”¨ç›´æ¥ Function Callingï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        try:
            # ä½¿ç”¨ bind_tools ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ï¼ˆåŸç”Ÿ Function Callingï¼‰
            if hasattr(self.llm.client, 'bind_tools'):
                # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
                model_with_tools = self.llm.client.bind_tools(langchain_tools)
                
                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    from langchain_core.messages import SystemMessage
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    from langchain_core.messages import SystemMessage
                    messages.insert(0, SystemMessage(content=system_prompt_text))
                
                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))
                
                # è°ƒç”¨æ¨¡å‹ï¼ˆéæµå¼ï¼Œè·å–å·¥å…·è°ƒç”¨ï¼‰
                response = model_with_tools.invoke(messages)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_messages = []
                    for tool_call in response.tool_calls:
                        tool_name = tool_call.get('name', '')
                        tool_args = tool_call.get('args', {})
                        tool_id = tool_call.get('id', '')
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·
                        tool_func = None
                        for tool in langchain_tools:
                            if tool.name == tool_name:
                                tool_func = tool
                                break
                        
                        if tool_func:
                            try:
                                # æ‰§è¡Œå·¥å…·
                                tool_result = tool_func.invoke(tool_args)
                                tool_messages.append(
                                    ToolMessage(
                                        content=str(tool_result),
                                        tool_call_id=tool_id
                                    )
                                )
                            except Exception as e:
                                tool_messages.append(
                                    ToolMessage(
                                        content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                                        tool_call_id=tool_id
                                    )
                                )
                                print(f"âŒ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
                        else:
                            tool_messages.append(
                                ToolMessage(
                                    content=f"å·¥å…·ä¸å­˜åœ¨: {tool_name}",
                                    tool_call_id=tool_id
                                )
                            )
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                    messages.append(response)
                    messages.extend(tool_messages)
                    
                    # æµå¼è·å–æœ€ç»ˆå›ç­”
                    for chunk in model_with_tools.stream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            yield chunk.content
                        elif isinstance(chunk, str):
                            yield chunk
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæµå¼è¿”å›å›ç­”
                    for chunk in model_with_tools.stream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            yield chunk.content
                        elif isinstance(chunk, str):
                            yield chunk
            else:
                # æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼
                raise AttributeError("æ¨¡å‹ä¸æ”¯æŒ bind_toolsï¼Œä½¿ç”¨ Agent æ¨¡å¼")
                
        except (AttributeError, Exception) as e:
            # Fallback: ä½¿ç”¨ Agent æ¨¡å¼
            # æ³¨æ„ï¼šLangChain Agent çš„æµå¼è¾“å‡ºæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œå…ˆè·å–å®Œæ•´å›ç­”ï¼Œç„¶åæµå¼è¾“å‡º
            print(f"âš ï¸  ç›´æ¥ Function Calling ä¸å¯ç”¨ï¼Œä½¿ç”¨ Agent æ¨¡å¼: {str(e)}")
            
            try:
                # ä½¿ç”¨ create_agent API
                agent = create_agent(
                    model=self.llm.client,
                    tools=langchain_tools,
                    system_prompt=system_prompt_text
                )
                
                # æ„å»ºæ¶ˆæ¯ï¼ˆåŒ…å«å†å²æ¶ˆæ¯ï¼‰
                messages = self._convert_history_to_messages(history)
                # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
                if messages and isinstance(messages[0], type(messages[0])) and hasattr(messages[0], 'content'):
                    from langchain_core.messages import SystemMessage
                    if isinstance(messages[0], SystemMessage):
                        messages[0].content = system_prompt_text
                    else:
                        messages.insert(0, SystemMessage(content=system_prompt_text))
                else:
                    from langchain_core.messages import SystemMessage
                    messages.insert(0, SystemMessage(content=system_prompt_text))
                
                # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                messages.append(HumanMessage(content=query))
                
                # æ‰§è¡Œ Agentï¼ˆéæµå¼ï¼‰
                result = agent.invoke({"messages": messages})
                messages = result.get("messages", [])
                ai_messages = [m for m in messages if isinstance(m, AIMessage)]
                
                if ai_messages:
                    # è·å–æœ€ç»ˆå›ç­”å†…å®¹
                    content = ai_messages[-1].content
                    # æµå¼è¾“å‡ºï¼ˆé€å­—ç¬¦ï¼Œè‡³å°‘æä¾›æµå¼ä½“éªŒï¼‰
                    # æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–å®ç°ï¼ŒçœŸæ­£çš„ Agent æµå¼éœ€è¦æ›´å¤æ‚çš„å¤„ç†
                    for char in content:
                        yield char
                else:
                    # å¦‚æœæ²¡æœ‰ AI æ¶ˆæ¯ï¼Œè¾“å‡ºæ•´ä¸ªç»“æœ
                    yield str(result)
                        
            except Exception as agent_error:
                print(f"âš ï¸  Agent æ‰§è¡Œå¤±è´¥: {str(agent_error)}ï¼Œé€€å›æ™®é€šæ¨¡å¼")
                if use_rag and self.rag_engine:
                    yield from self._chat_with_rag_stream(query, history=history)
                else:
                    yield from self._chat_with_llm_stream(query, history=history)


__all__ = ["ChatEngine", "SlashCommandRegistry"]

