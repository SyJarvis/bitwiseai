# -*- coding: utf-8 -*-
"""
Milvus å‘é‡æ•°æ®åº“

æœ¬åœ° Milvus å‘é‡æ•°æ®åº“å®ç°
"""
import os
import time
import hashlib
from typing import List, Optional, Dict, Any

try:
    from pymilvus import MilvusClient
    MILVUS_AVAILABLE = True
except ImportError:
    MILVUS_AVAILABLE = False


class MilvusDB:
    """
    Milvus å‘é‡æ•°æ®åº“

    åŸºäº pymilvus çš„æœ¬åœ°æ–‡ä»¶æ¨¡å¼
    """

    def __init__(
        self,
        db_file: str,
        embedding_model,
        collection_name: str = "polarisrag",
        embedding_dim: int = 1024
    ):
        """
        åˆå§‹åŒ– Milvus æ•°æ®åº“

        Args:
            db_file: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            embedding_model: åµŒå…¥æ¨¡å‹
            collection_name: é›†åˆåç§°
            embedding_dim: å‘é‡ç»´åº¦
        """
        if not MILVUS_AVAILABLE:
            raise ImportError("è¯·å®‰è£… pymilvus: pip install pymilvus")

        self.db_file = db_file
        self.embedding_model = embedding_model
        self.collection_name = collection_name
        self.embedding_dim = embedding_dim

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(db_file), exist_ok=True)

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = MilvusClient(uri=db_file)

        # åˆ›å»ºé›†åˆ
        self._create_collection()

    def _create_collection(self):
        """åˆ›å»ºé›†åˆï¼ˆæ”¯æŒå…ƒæ•°æ®å­—æ®µï¼‰"""
        if self.client.has_collection(self.collection_name):
            # æ£€æŸ¥ç°æœ‰é›†åˆçš„schemaï¼Œå¦‚æœç¼ºå°‘å…ƒæ•°æ®å­—æ®µï¼Œéœ€è¦è¿ç§»
            return

        # MilvusClientçš„ç®€åŒ–APIï¼šä½¿ç”¨create_collectionï¼Œæ”¯æŒåŠ¨æ€å­—æ®µ
        # é€šè¿‡enable_dynamic_field=Trueï¼Œå¯ä»¥è‡ªåŠ¨å¤„ç†é¢å¤–çš„å…ƒæ•°æ®å­—æ®µ
        # auto_id é»˜è®¤ä¸º Trueï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®
        try:
            self.client.create_collection(
                collection_name=self.collection_name,
                dimension=self.embedding_dim,
                metric_type="IP",
                consistency_level="Strong",
                enable_dynamic_field=True  # å¯ç”¨åŠ¨æ€å­—æ®µä»¥æ”¯æŒå…ƒæ•°æ®
            )
            print(f"âœ“ é›†åˆ '{self.collection_name}' å·²åˆ›å»ºï¼ˆæ”¯æŒå…ƒæ•°æ®ï¼‰")
        except Exception as e:
            # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸºæœ¬æ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            print(f"âš ï¸  åˆ›å»ºé›†åˆæ—¶å‡ºé”™: {e}ï¼Œå°è¯•ä½¿ç”¨åŸºæœ¬æ–¹å¼")
            try:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    dimension=self.embedding_dim,
                    metric_type="IP",
                    consistency_level="Strong"
                )
                print(f"âœ“ é›†åˆ '{self.collection_name}' å·²åˆ›å»ºï¼ˆåŸºæœ¬æ¨¡å¼ï¼‰")
            except Exception as e2:
                print(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e2}")

    def add_texts(self, texts: List[str]) -> int:
        """
        æ·»åŠ æ–‡æœ¬åˆ°å‘é‡æ•°æ®åº“ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            æ’å…¥çš„æ–‡æœ¬æ•°é‡
        """
        if not texts:
            return 0

        # ç”ŸæˆåµŒå…¥
        vectors = self.embedding_model.embed_documents(texts)

        # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨é»˜è®¤å…ƒæ•°æ®ï¼‰
        metadata = []
        current_time = time.time()
        for i, text in enumerate(texts):
            metadata.append({
                "source_file": "",
                "file_hash": "",
                "chunk_index": i,
                "chunk_total": len(texts),
                "timestamp": current_time,
                "text_length": len(text)
            })

        return self.add_texts_with_metadata(texts, metadata)

    def add_texts_with_metadata(self, texts: List[str], metadata: List[Dict[str, Any]]) -> int:
        """
        æ·»åŠ æ–‡æœ¬åˆ°å‘é‡æ•°æ®åº“ï¼ˆå¸¦å…ƒæ•°æ®ï¼‰

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            metadata: å…ƒæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - source_file: æºæ–‡ä»¶è·¯å¾„
                - file_hash: æ–‡ä»¶å“ˆå¸Œå€¼
                - chunk_index: åˆ‡åˆ†å—ç´¢å¼•
                - chunk_total: æ€»åˆ‡åˆ†å—æ•°
                - timestamp: æ—¶é—´æˆ³
                - text_length: æ–‡æœ¬é•¿åº¦

        Returns:
            æ’å…¥çš„æ–‡æœ¬æ•°é‡
        """
        if not texts:
            return 0

        if len(texts) != len(metadata):
            raise ValueError("æ–‡æœ¬åˆ—è¡¨å’Œå…ƒæ•°æ®åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸€è‡´")

        # ç”ŸæˆåµŒå…¥ï¼ˆæ·»åŠ è¿›åº¦æ˜¾ç¤ºå’Œæ‰¹é‡å¤„ç†ï¼‰
        print(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡...")
        try:
            # æ‰¹é‡å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†è¿‡å¤šæ–‡æœ¬
            batch_size = 100  # æ¯æ‰¹å¤„ç†100ä¸ªæ–‡æœ¬
            all_vectors = []
            total_batches = (len(texts) + batch_size - 1) // batch_size
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_num = i // batch_size + 1
                print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_texts)} ä¸ªæ–‡æœ¬)...", end='\r')
                batch_vectors = self.embedding_model.embed_documents(batch_texts)
                all_vectors.extend(batch_vectors)
            
            vectors = all_vectors
            print(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ ({len(vectors)} ä¸ªå‘é‡)                    ")
        except Exception as e:
            error_msg = str(e)
            if "No embedding data received" in error_msg or "data" in error_msg.lower():
                raise ValueError(
                    f"åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥ï¼šAPI æœªè¿”å›æ•°æ®ã€‚\n"
                    f"è¯·æ£€æŸ¥ï¼š\n"
                    f"  1) Embedding API é…ç½®æ˜¯å¦æ­£ç¡®\n"
                    f"  2) API åœ°å€å’Œ Key æ˜¯å¦æœ‰æ•ˆ\n"
                    f"  3) ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                    f"  4) æ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"
                ) from e
            raise

        # å‡†å¤‡æ•°æ®
        # æ³¨æ„ï¼šå¦‚æœé›†åˆæ²¡æœ‰è®¾ç½® auto_idï¼Œéœ€è¦æ‰‹åŠ¨ç”Ÿæˆ id
        data = []
        import random
        base_id = int(time.time() * 1000)  # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºåŸºç¡€ID
        
        for idx, (text, vector, meta) in enumerate(zip(texts, vectors, metadata)):
            # ç”Ÿæˆå”¯ä¸€IDï¼ˆæ—¶é—´æˆ³ + ç´¢å¼• + éšæœºæ•°ï¼‰
            doc_id = base_id + idx * 1000 + random.randint(0, 999)
            
            data.append({
                "id": doc_id,  # æ‰‹åŠ¨ç”ŸæˆID
                "vector": vector,
                "text": text,
                "source_file": meta.get("source_file", ""),
                "file_name": meta.get("file_name", ""),  # æ–°å¢
                "file_name_keywords": meta.get("file_name_keywords", ""),  # æ–°å¢
                "file_hash": meta.get("file_hash", ""),
                "chunk_index": meta.get("chunk_index", 0),
                "chunk_total": meta.get("chunk_total", 1),
                "timestamp": meta.get("timestamp", time.time()),
                "text_length": meta.get("text_length", len(text))
            })

        # æ’å…¥æ•°æ®
        try:
            insert_res = self.client.insert(
                collection_name=self.collection_name,
                data=data
            )

            # æŒä¹…åŒ–
            try:
                self.client.flush()
            except Exception:
                pass

            return insert_res.get("insert_count", len(texts))
        except Exception as e:
            print(f"âš ï¸  æ’å…¥æ•°æ®å¤±è´¥: {e}")
            return 0

    def search(
        self,
        query: str,
        top_k: int = 5
    ) -> str:
        """
        æœç´¢ç›¸ä¼¼æ–‡æœ¬ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æœ¬å†…å®¹ï¼ˆç”¨æ¢è¡Œç¬¦è¿æ¥ï¼‰
        """
        results = self.search_with_metadata(query, top_k)
        return "\n".join([r["text"] for r in results])

    def search_with_metadata(
        self,
        query: str,
        top_k: int = 5,
        output_fields: Optional[List[str]] = None,
        filter_expr: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æœ¬ï¼ˆè¿”å›å…ƒæ•°æ®ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            output_fields: éœ€è¦è¿”å›çš„å­—æ®µåˆ—è¡¨ï¼Œé»˜è®¤è¿”å›æ‰€æœ‰å­—æ®µ
            filter_expr: Milvus filterè¡¨è¾¾å¼ï¼Œç”¨äºè¿‡æ»¤æ–‡æ¡£ï¼ˆä¾‹å¦‚ï¼š'source_file in ["doc1.md", "doc2.md"]'ï¼‰

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«textå’Œå…ƒæ•°æ®
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.embed_text(query)

        # è®¾ç½®è¾“å‡ºå­—æ®µ
        if output_fields is None:
            output_fields = ["text", "source_file", "file_hash", "chunk_index", "chunk_total", "timestamp", "file_name"]

        # æœç´¢
        try:
            # MilvusClient.search çš„ç­¾åï¼šsearch(collection_name, data, filter='', limit=10, output_fields=None, ...)
            # æ³¨æ„ï¼šå‚æ•°åæ˜¯ filter è€Œä¸æ˜¯ expr
            search_res = self.client.search(
                collection_name=self.collection_name,
                data=[query_vector],
                filter=filter_expr if filter_expr else '',  # filterè¡¨è¾¾å¼ï¼Œä½¿ç”¨ filter å‚æ•°
                limit=top_k,
                output_fields=output_fields
            )

            # è§£æç»“æœ
            if not search_res or not search_res[0]:
                return []

            results = []
            for res in search_res[0]:
                # å…¼å®¹ä¸åŒçš„è¿”å›æ ¼å¼
                # pymilvuså¯èƒ½è¿”å› {"entity": {...}} æˆ–ç›´æ¥è¿”å›å­—æ®µ
                if isinstance(res, dict):
                    entity = res.get("entity", res)  # å¦‚æœæ²¡æœ‰entityå­—æ®µï¼Œç›´æ¥ä½¿ç”¨res
                    result = {
                        "text": entity.get("text", ""),
                        "score": res.get("distance", res.get("score", 0.0)),
                        "source_file": entity.get("source_file", ""),
                        "file_hash": entity.get("file_hash", ""),
                        "chunk_index": entity.get("chunk_index", 0),
                        "chunk_total": entity.get("chunk_total", 1),
                        "timestamp": entity.get("timestamp", 0.0)
                    }
                    results.append(result)

            return results
        except Exception as e:
            print(f"âš ï¸  æœç´¢å¤±è´¥: {e}")
            return []

    def search_similar_vectors(
        self,
        vectors: List[List[float]],
        threshold: float = 0.85,
        top_k: int = 10
    ) -> List[List[Dict[str, Any]]]:
        """
        æœç´¢ç›¸ä¼¼å‘é‡ï¼ˆç”¨äºå»é‡ï¼‰

        Args:
            vectors: æŸ¥è¯¢å‘é‡åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            top_k: æ¯ä¸ªå‘é‡è¿”å›çš„æœ€å¤§ç»“æœæ•°

        Returns:
            æ¯ä¸ªæŸ¥è¯¢å‘é‡çš„ç›¸ä¼¼ç»“æœåˆ—è¡¨
        """
        if not vectors:
            return []

        try:
            # åœ¨Milvusä¸­æœç´¢
            search_res = self.client.search(
                collection_name=self.collection_name,
                data=vectors,
                limit=top_k,
                output_fields=["text", "source_file", "file_hash", "chunk_index"]
            )

            # è½¬æ¢IPè·ç¦»ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
            # Milvusä½¿ç”¨IPï¼ˆå†…ç§¯ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            results = []
            for res_list in search_res:
                similar_items = []
                for res in res_list:
                    # IPè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆè¿‘ä¼¼ï¼‰
                    # å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼ŒIP â‰ˆ ä½™å¼¦ç›¸ä¼¼åº¦
                    distance = res.get("distance", res.get("score", 0.0))
                    # å‡è®¾å‘é‡å·²å½’ä¸€åŒ–ï¼ŒIPå€¼æ¥è¿‘ä½™å¼¦ç›¸ä¼¼åº¦
                    # å¦‚æœå‘é‡æœªå½’ä¸€åŒ–ï¼Œéœ€è¦æ›´å¤æ‚çš„è½¬æ¢
                    # IP metric: å€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼Œé€šå¸¸èŒƒå›´åœ¨[-1, 1]ï¼ˆå½’ä¸€åŒ–å‘é‡ï¼‰
                    # è½¬æ¢ä¸º[0, 1]èŒƒå›´çš„ç›¸ä¼¼åº¦
                    similarity = max(0.0, min(1.0, (distance + 1.0) / 2.0))  # å°†[-1,1]æ˜ å°„åˆ°[0,1]
                    
                    if similarity >= threshold:
                        # å…¼å®¹ä¸åŒçš„è¿”å›æ ¼å¼
                        entity = res.get("entity", res)
                        similar_items.append({
                            "text": entity.get("text", ""),
                            "score": similarity,
                            "source_file": entity.get("source_file", ""),
                            "file_hash": entity.get("file_hash", ""),
                            "chunk_index": entity.get("chunk_index", 0)
                        })
                results.append(similar_items)

            return results
        except Exception as e:
            print(f"âš ï¸  ç›¸ä¼¼å‘é‡æœç´¢å¤±è´¥: {e}")
            return [[] for _ in vectors]

    def hybrid_search(
        self,
        query: str,
        top_k: int = 5,
        use_keyword: bool = True,
        vector_weight: float = 0.7,
        keyword_weight: float = 0.3,
        filter_expr: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæ£€ç´¢ï¼ˆå‘é‡æœç´¢ + å…³é”®è¯æœç´¢ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_keyword: æ˜¯å¦ä½¿ç”¨å…³é”®è¯æœç´¢
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡
            filter_expr: Milvus filterè¡¨è¾¾å¼ï¼Œç”¨äºè¿‡æ»¤æ–‡æ¡£

        Returns:
            åˆå¹¶åçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # å‘é‡æœç´¢ï¼ˆä½¿ç”¨filterï¼‰
        vector_results = self.search_with_metadata(query, top_k=top_k * 2, filter_expr=filter_expr)

        if not use_keyword:
            return vector_results[:top_k]

        # å…³é”®è¯æœç´¢ï¼ˆç®€å•å®ç°ï¼šåœ¨æ–‡æœ¬ä¸­æœç´¢å…³é”®è¯ï¼Œä¹Ÿä½¿ç”¨ç›¸åŒçš„filterï¼‰
        keyword_results = self._keyword_search(query, top_k=top_k * 2, filter_expr=filter_expr)

        # åˆå¹¶ç»“æœ
        combined = self._merge_search_results(
            vector_results, keyword_results,
            vector_weight, keyword_weight
        )

        return combined[:top_k]

    def _keyword_search(self, query: str, top_k: int = 10, filter_expr: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        å…³é”®è¯æœç´¢ï¼ˆç®€å•å®ç°ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filter_expr: Milvus filterè¡¨è¾¾å¼ï¼Œç”¨äºè¿‡æ»¤æ–‡æ¡£

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # ç®€å•å®ç°ï¼šè·å–æ‰€æœ‰æ–‡æ¡£ï¼ŒæŒ‰å…³é”®è¯åŒ¹é…åº¦æ’åº
        # æ³¨æ„ï¼šå¯¹äºå¤§æ•°æ®é›†ï¼Œè¿™éœ€è¦ä¼˜åŒ–ï¼ˆå¦‚ä½¿ç”¨å€’æ’ç´¢å¼•ï¼‰
        try:
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            query_params = {
                "collection_name": self.collection_name,
                "filter": filter_expr if filter_expr else "",
                "limit": min(1000, top_k * 20),  # é™åˆ¶æŸ¥è¯¢æ•°é‡
                "output_fields": ["text", "source_file", "file_hash", "chunk_index", "chunk_total", "timestamp", "file_name"]
            }
            
            # è·å–æ‰€æœ‰æ–‡æ¡£ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…æ€§èƒ½é—®é¢˜ï¼‰
            query_result = self.client.query(**query_params)

            if not query_result:
                return []

            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            query_words = set(query.lower().split())
            scored_results = []
            for item in query_result:
                text = item.get("text", "").lower()
                # è®¡ç®—åŒ¹é…çš„å…³é”®è¯æ•°é‡
                matched_words = sum(1 for word in query_words if word in text)
                if matched_words > 0:
                    score = matched_words / len(query_words) if query_words else 0.0
                    scored_results.append({
                        "text": item.get("text", ""),
                        "score": score,
                        "source_file": item.get("source_file", ""),
                        "file_hash": item.get("file_hash", ""),
                        "chunk_index": item.get("chunk_index", 0),
                        "chunk_total": item.get("chunk_total", 1),
                        "timestamp": item.get("timestamp", 0.0)
                    })

            # æŒ‰åˆ†æ•°æ’åº
            scored_results.sort(key=lambda x: x["score"], reverse=True)
            return scored_results[:top_k]
        except Exception as e:
            print(f"âš ï¸  å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []

    def _merge_search_results(
        self,
        vector_results: List[Dict[str, Any]],
        keyword_results: List[Dict[str, Any]],
        vector_weight: float = 0.7,
        keyword_weight: float = 0.3
    ) -> List[Dict[str, Any]]:
        """
        åˆå¹¶å‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢ç»“æœ

        Args:
            vector_results: å‘é‡æœç´¢ç»“æœ
            keyword_results: å…³é”®è¯æœç´¢ç»“æœ
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡

        Returns:
            åˆå¹¶åçš„ç»“æœåˆ—è¡¨
        """
        # åˆ›å»ºæ–‡æœ¬åˆ°ç»“æœçš„æ˜ å°„
        combined_map = {}

        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for i, result in enumerate(vector_results):
            text = result["text"]
            if text not in combined_map:
                combined_map[text] = result.copy()
                combined_map[text]["combined_score"] = vector_weight * (1.0 - result.get("score", 0.0))
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°åˆ†æ•°
                combined_map[text]["combined_score"] += vector_weight * (1.0 - result.get("score", 0.0))

        # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
        for result in keyword_results:
            text = result["text"]
            if text not in combined_map:
                combined_map[text] = result.copy()
                combined_map[text]["combined_score"] = keyword_weight * result.get("score", 0.0)
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°åˆ†æ•°
                combined_map[text]["combined_score"] += keyword_weight * result.get("score", 0.0)

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰åˆ†æ•°æ’åº
        combined = list(combined_map.values())
        combined.sort(key=lambda x: x.get("combined_score", 0.0), reverse=True)

        return combined

    def clear(self):
        """
        æ¸…ç©ºé›†åˆ
        """
        if self.client.has_collection(self.collection_name):
            self.client.drop_collection(self.collection_name)
            self._create_collection()

    def count(self) -> int:
        """
        è·å–é›†åˆä¸­çš„æ–‡æ¡£æ•°é‡

        Returns:
            æ–‡æ¡£æ•°é‡
        """
        if not self.client.has_collection(self.collection_name):
            return 0

        try:
            stats = self.client.get_collection_stats(self.collection_name)
            return stats.get("row_count", 0)
        except Exception:
            return 0


__all__ = ["MilvusDB"]
